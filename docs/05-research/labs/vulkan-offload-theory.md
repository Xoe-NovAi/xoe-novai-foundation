**Meta**: Grok Account: Arcana.Novai; Project: Xoe-NovAi - v0.1.3 â†’ 0.1.4 Phase 1  
Chat Session: Vulkan Offload Implementation Guide  
Timestamp: January 27, 2026 ~20:15 AST

# ðŸš€ Complete Vulkan Offload Guide for Xoe-NovAi  
Ryzen 7 5700U (Vega 8 iGPU) â€“ Production Deployment 2026

### Current Status (mid-January 2026)

Vulkan backend in llama.cpp reached good production stability around build ~b3670â€“b3750 (late 2025).  
Many Ryzen 5000G / 5700U / 6800U users report **real-world gains between 1.35Ã— and 2.1Ã—** on prompt processing speed (especially strong on 4kâ€“8k context), with generation speed improvement usually **15â€“45%**.

Memory usage is typically **similar or 100â€“400 MB higher** than pure CPU path (due to VRAM allocation + staging buffers).

### Realistic Expectations on 5700U + Vega 8

| Workload                       | CPU-only (baseline) | Vulkan partial offload | Gain range | Most common gain |
| ------------------------------ | ------------------- | ---------------------- | ---------- | ---------------- |
| Prompt eval (4k context)       | 12â€“18 t/s           | 18â€“38 t/s              | +30â€“110%   | ~+45â€“65%         |
| Generation (temperature 0.7)   | 15â€“22 t/s           | 18â€“32 t/s              | +10â€“55%    | ~+20â€“35%         |
| Memory usage (Q5_K_XL ~4.1 GB) | ~4.8â€“5.4 GB         | ~5.0â€“5.9 GB            | +0â€“0.8 GB  | ~+250â€“450 MB     |

**Sweet spot** for 5700U:  
`n_gpu_layers = 22â€“35` (partial offload)  
Complete offload (`-ngl 99`) usually gives worse performance due to PCIe bottleneck + VRAM thrashing.

### Step-by-Step Production Implementation Guide

#### 1. Host Prerequisites (Ubuntu 22.04 / 24.04 recommended)

```bash
# Install Vulkan ecosystem + latest mesa (critical!)
sudo apt update
sudo apt install -y \
    vulkan-tools \
    mesa-vulkan-drivers \
    libvulkan-dev \
    libvulkan1 \
    libgl1-mesa-dri \
    mesa-va-drivers

# Verify Vulkan sees Vega 8
vulkaninfo --summary | grep deviceName
# Should show something like:
# deviceName   = AMD Radeon Graphics (RADV VANGOGH)

# Optional â€“ check layers
vulkaninfo | grep deviceLayer
```

#### 2. Modify Podmanfile.api (multi-stage build)

```dockerfile
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STAGE: builder
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FROM python:3.12-slim AS builder

# Install Vulkan build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    vulkan-tools \
    libvulkan-dev \
    mesa-vulkan-drivers \
    pkg-config \
    cmake \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Build llama-cpp-python with Vulkan support
ARG LLAMA_VULKAN=OFF
ENV CMAKE_ARGS="-DLLAMA_VULKAN=${LLAMA_VULKAN} ${CMAKE_ARGS}"

# ... rest of your uv / pip install layer ...

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STAGE: runtime
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FROM python:3.12-slim

# Runtime Vulkan libraries (much smaller footprint)
RUN apt-get update && apt-get install -y --no-install-recommends \
    libvulkan1 \
    mesa-vulkan-drivers \
    && rm -rf /var/lib/apt/lists/*

COPY --from=builder /usr/local/lib/python3.12/site-packages /usr/local/lib/python3.12/site-packages

# Important runtime env for stability
ENV VK_ICD_FILENAMES=/usr/share/vulkan/icd.d/radeon_icd.x86_64.json
ENV LIBVA_DRIVER_NAME=radeonsi
```

**Build command** (with Vulkan):
```bash
podman compose build --build-arg LLAMA_VULKAN=ON rag
```

#### 3. Configuration & Safety Rails (.env / config.toml)

```bash
# .env â€“ recommended safe defaults
LLAMA_VULKAN_ENABLED=false           # change to true AFTER testing
LLAMA_VULKAN_LAYERS=28               # start conservative: 22â€“35
LLAMA_VULKAN_FALLBACK_LAYERS=0       # emergency fallback if Vulkan crashes
LLAMA_VULKAN_MEMORY_BUDGET_MB=1800   # conservative VRAM cap (~1.8 GB)
```

```toml
# config.toml
[vulkan]
enabled = false
layers = 28
memory_budget_mb = 1800
emergency_fallback = true
```

#### 4. Safe Loading Logic in dependencies.py

```python
import os
from llama_cpp import Llama

def get_llm():
    vulkan_enabled = os.getenv("LLAMA_VULKAN_ENABLED", "false").lower() == "true"
    requested_layers = int(os.getenv("LLAMA_VULKAN_LAYERS", "0"))

    try:
        if vulkan_enabled:
            llm = Llama(
                model_path=CONFIG["models"]["llm_path"],
                n_gpu_layers=requested_layers,
                n_threads=6,
                f16_kv=True,
                use_mlock=True,
                # Safety: limit context if VRAM is tight
                n_ctx=min(4096, CONFIG["models"]["llm_context_window"]),
                verbose=False
            )
            print(f"[Vulkan] Using {requested_layers} layers")
        else:
            llm = Llama(
                model_path=CONFIG["models"]["llm_path"],
                n_gpu_layers=0,
                n_threads=6,
                f16_kv=True,
                use_mlock=True
            )
    except RuntimeError as e:
        if "Vulkan" in str(e) and vulkan_enabled:
            print(f"[Vulkan fallback] {e} â†’ falling back to CPU")
            return get_llm_cpu_fallback()  # implement pure CPU path
        raise

    return llm
```

#### 5. Validation & Benchmark Protocol (Critical!)

**Must run before production use**

```bash
# 1. Baseline CPU
LLAMA_VULKAN_ENABLED=false make benchmark

# 2. Vulkan partial
LLAMA_VULKAN_ENABLED=true LLAMA_VULKAN_LAYERS=28 make benchmark

# 3. Stress test (watch memory & temperature)
for i in {1..5}; do
  python -c "from dependencies import get_llm; llm = get_llm(); print(llm('Long prompt...' * 200))"
done
```

**Acceptance criteria**:
- â‰¥ +20% prompt eval speed
- No more than +700 MB memory
- No crashes after 30 min stress
- Temperature stays reasonable (<82Â°C sustained)

**Recommended starting points for 5700U**:
- First try: 24â€“28 layers
- If stable & fast: try 32â€“35
- If unstable/crashes: drop to 18â€“22 or disable

**Common failure modes & fixes** (2026 community reports):

| Symptom                           | Most likely cause                | Fix                         |
| --------------------------------- | -------------------------------- | --------------------------- |
| Instant crash on load             | Missing mesa-vulkan-drivers      | Reinstall mesa packages     |
| Very slow after initial prompt    | Too many layers â†’ thrashing      | Reduce n_gpu_layers by 8â€“10 |
| +800â€“1200 MB extra memory         | Over-aggressive layers           | Cap at 28â€“32                |
| "Vulkan device lost" after 10 min | VRAM fragmentation / power limit | Lower layers + enable mlock |

**Bottom line (Jan 2026)**:  
Vulkan is currently the **single highest-ROI upgrade** you can make on 5700U-class hardware for LLM inference â€” provided you validate carefully and start conservatively.

Would you like me to prepare the exact PR diff structure / file changes for this Vulkan integration (Podmanfile + dependencies.py + config + safety rails)?