# XNAi Foundation — Model Router Configuration
# ================================================
# SINGLE SOURCE OF TRUTH for all AI model and provider configuration.
# All agents, scripts, Python modules, and documentation derive from this file.
#
# Generated by: Cline VSCodium Extension
# Model: claude-sonnet-4-6 (limited-time special free tier)
# Branch: main | Date: 2026-02-18
# Version: 1.0.0
#
# Usage:
#   Python: from app.XNAi_rag_app.model_router import ModelRouter; router = ModelRouter()
#   CLI:    ./scripts/model-status.sh
#   Docs:   docs/tutorials/FREE-AI-PROVIDERS-COMPLETE-GUIDE.md

metadata:
  version: "1.0.0"
  last_updated: "2026-02-18"
  authoritative_source: "https://openrouter.ai/api/v1/models"
  snapshot_dir: "expert-knowledge/model-snapshots/"
  update_command: "./scripts/update-model-intelligence.sh"

# ─── PROVIDER TIERS ──────────────────────────────────────────────────────────
# Ordered by priority: TIER 1 is tried first, fallback down the list.
# Cost = 0 means free (no API key billing) but may require auth.

providers:

  # ── TIER 1: OpenCode CLI + Antigravity Auth Plugin ────────────────────────
  # Antigravity (opencode-antigravity-auth@latest) is an OAuth PLUGIN that runs
  # INSIDE OpenCode CLI. It is NOT a separate CLI or agent.
  # It unlocks premium GitHub OAuth-authenticated models within OpenCode.
  # OpenCode CLI status: ACTIVE (upstream repo archived by original maintainers —
  # this does NOT affect use; fork planned for XNAi custom TUI).
  - id: opencode_antigravity
    name: "OpenCode CLI + Antigravity Auth Plugin (GitHub OAuth → Premium Models)"
    tier: 1
    cost: free
    requires_auth: true
    auth_type: "github_oauth"                 # Antigravity uses GitHub OAuth (NOT Google)
    auth_config_path: "~/.config/opencode/antigravity-accounts.json"
    plugin: "opencode-antigravity-auth@latest"
    cli: "opencode"                           # OpenCode is the host CLI
    daily_limit: "generous (3-account rotation)"
    rate_limit_strategy: "auto-rotate (3 GitHub accounts)"
    sovereignty: partial  # uses GitHub OAuth but models run externally
    setup_url: "expert-knowledge/research/ANTIGRAVITY-OAUTH-QUICKGUIDE-2026-02-18.md"
    taxonomy_note: "See configs/agent-identity.yaml opencode.plugin_antigravity for full spec"
    models:
      - id: "google/antigravity-gemini-3-pro"
        name: "Gemini 3 Pro (Antigravity)"
        context_length: 1000000
        max_output: 65536
        cost_per_mtok_in: 0
        cost_per_mtok_out: 0
        capabilities: [reasoning, coding, analysis, 1M_context, multimodal]
        thinking_variant: "--variant=high"
        best_for: [full_codebase_audit, large_doc_synthesis, architecture]

      - id: "google/antigravity-gemini-3-flash"
        name: "Gemini 3 Flash (Antigravity)"
        context_length: 1000000
        max_output: 65536
        cost_per_mtok_in: 0
        cost_per_mtok_out: 0
        capabilities: [fast, 1M_context, multimodal]
        best_for: [context_fallback, quick_large_ctx, document_ingestion]

      - id: "google/antigravity-claude-sonnet-4-6"
        name: "Claude Sonnet 4.6 (Antigravity)"
        context_length: 200000
        max_output: 64000
        cost_per_mtok_in: 0
        cost_per_mtok_out: 0
        capabilities: [reasoning, coding, instruction_following]
        training_cutoff: "2026-01"
        best_for: [daily_coding, bug_fixes, refactoring]

      - id: "google/antigravity-claude-opus-4-6-thinking"
        name: "Claude Opus 4.6 Thinking (Antigravity)"
        context_length: 200000
        max_output: 128000
        cost_per_mtok_in: 0
        cost_per_mtok_out: 0
        capabilities: [deep_reasoning, extended_thinking, architecture]
        thinking_variant: "--variant=max"
        training_cutoff: "2025-08"
        best_for: [architecture_decisions, complex_reasoning, long_doc_generation]

      - id: "google/antigravity-claude-opus-4-5-thinking"
        name: "Claude Opus 4.5 Thinking (Antigravity)"
        context_length: 200000
        max_output: 32000
        cost_per_mtok_in: 0
        cost_per_mtok_out: 0
        capabilities: [deep_reasoning, extended_thinking]
        thinking_variant: "--variant=max"
        best_for: [architecture_decisions, complex_reasoning]

      - id: "google/antigravity-claude-sonnet-4-5"
        name: "Claude Sonnet 4.5 (Antigravity)"
        context_length: 200000
        max_output: 64000
        cost_per_mtok_in: 0
        cost_per_mtok_out: 0
        capabilities: [reasoning, coding]
        best_for: [daily_coding, bug_fixes]

  # ── TIER 2: Gemini CLI (separate 25 req/day free quota) ──────────────────
  - id: gemini_cli
    name: "Gemini CLI"
    tier: 2
    cost: free
    requires_auth: true
    auth_type: "google_account"
    cli: "gemini"
    daily_limit: "25 requests/day (Pro), higher for Flash"
    rate_limit_strategy: "built-in backoff"
    sovereignty: partial
    install: "npm install -g @google/gemini-cli"
    setup_url: "https://github.com/google-gemini/gemini-cli"
    rules_file: "GEMINI.md"
    models:
      - id: "gemini-2.5-pro"
        name: "Gemini 2.5 Pro"
        context_length: 1000000
        max_output: 65536
        cost_per_mtok_in: 0  # free quota
        capabilities: [1M_context, multimodal, search_grounding, reasoning]
        unique_feature: "native Google Search grounding"
        best_for: [full_codebase_analysis, research_synthesis, web_grounded_research]

      - id: "gemini-2.5-flash"
        name: "Gemini 2.5 Flash"
        context_length: 1000000
        max_output: 65536
        cost_per_mtok_in: 0  # free quota (higher limit than Pro)
        capabilities: [fast, 1M_context, multimodal]
        best_for: [quick_large_context, doc_ingestion]

  # ── TIER 3: GitHub Copilot CLI (free Copilot plan) ───────────────────────
  - id: copilot_cli
    name: "GitHub Copilot CLI"
    tier: 3
    cost: free  # requires active Copilot subscription (free plan qualifies)
    requires_auth: true
    auth_type: "github_oauth"
    cli: "copilot"
    install_path: "~/.copilot/"
    cli_version: "v0.0.411"
    daily_limit: "~50 chat messages/day, 2000 completions/month"
    note: "Use 'copilot' not 'gh copilot' — the latter is only a basic command helper"
    session_state_path: "~/.copilot/session-state/"
    sovereignty: partial
    models:
      - id: "claude-haiku-4.5"
        name: "Claude Haiku 4.5 (Copilot)"
        context_length: 200000
        cost_per_mtok_in: 0
        capabilities: [fast, routing, simple_tasks, github_native]
        best_for: [quick_fixes, github_tasks, routing]

      - id: "gemini-3-flash-preview"
        name: "Gemini 3 Flash Preview (Copilot label)"
        context_length: 1000000
        cost_per_mtok_in: 0
        capabilities: [1M_context, fast]
        note: "Copilot internal label — likely Gemini 2.5 Flash class"
        best_for: [large_context_when_antigravity_down]

      - id: "gpt-5-mini"
        name: "GPT-5 Mini (Copilot label)"
        context_length: 128000
        cost_per_mtok_in: 0
        capabilities: [general, fast]
        note: "Copilot internal label — NOT public OpenAI GPT-5"
        best_for: [general_tasks]

  # ── TIER 4: OpenCode Built-in Free Models ─────────────────────────────────
  - id: opencode_builtin
    name: "OpenCode Built-in Free Models"
    tier: 4
    cost: free
    requires_auth: false
    cli: "opencode"
    note: "True zero-auth free models — rate-limited shared pool"
    sovereignty: partial  # models run on OpenCode servers
    fallback_order: [big-pickle, kimi-k2.5-free, gpt-5-nano, minimax-m2.5-free, glm-5-free]
    models:
      - id: "opencode/big-pickle"
        name: "Big Pickle"
        context_length: 200000
        max_output: 128000
        cost_per_mtok_in: 0
        capabilities: [general, reasoning, coding]
        note: "Best quality among OpenCode built-ins; underlying model undisclosed"
        underlying_openrouter_equivalent: null  # opaque
        best_for: [rate_limit_fallback_primary]

      - id: "opencode/kimi-k2.5-free"
        name: "Kimi K2.5 (OpenCode free tier)"
        context_length: 262144
        max_output: 128000
        cost_per_mtok_in: 0
        capabilities: [coding, 200K_context, multimodal, agentic]
        underlying_openrouter_id: "moonshotai/kimi-k2.5"
        swe_bench_score: 76.8
        note: "CONFIRMED REAL — moonshotai/kimi-k2.5, 1T MoE, 32B active"
        best_for: [research_synthesis, agentic_tasks, large_context_free]

      - id: "opencode/gpt-5-nano"
        name: "GPT-5 Nano (OpenCode label)"
        context_length: 400000
        max_output: 128000
        cost_per_mtok_in: 0
        note: "NOT public GPT-5 — OpenCode internal label only. LARGEST free context."
        best_for: [400K_context_tasks]

      - id: "opencode/minimax-m2.5-free"
        name: "MiniMax M2.5 (OpenCode free tier)"
        context_length: 204800
        max_output: 128000
        cost_per_mtok_in: 0
        capabilities: [fast, coding]
        underlying_openrouter_id: "minimax/minimax-m2.5"
        swe_bench_score: 80.2
        note: "CONFIRMED REAL — 80.2% SWE-Bench, strongest coding benchmark"
        best_for: [fast_coding, prototyping]

      - id: "opencode/glm-5-free"
        name: "GLM-5 (OpenCode free tier)"
        context_length: 200000
        max_output: 128000
        cost_per_mtok_in: 0
        capabilities: [multilingual, structured, logical]
        underlying_openrouter_id: "z-ai/glm-5"
        note: "CONFIRMED REAL — z-ai/glm-5 (Zhipu AI). Most rate-limited."
        best_for: [multilingual_tasks, structured_logic, last_resort]

  # ── TIER 5: OpenRouter (paid, per-use pricing) ────────────────────────────
  - id: openrouter
    name: "OpenRouter (per-use pricing)"
    tier: 5
    cost: paid
    requires_auth: true
    auth_type: "api_key"
    env_var: "OPENROUTER_API_KEY"
    api_url: "https://openrouter.ai/api/v1"
    model_catalog_url: "https://openrouter.ai/api/v1/models"
    free_models_available: true  # ~20+ free models with :free suffix
    sovereignty: partial
    note: "367+ models from 50+ providers. Many have :free variants."
    key_models:
      - id: "moonshotai/kimi-k2.5"
        cost_per_mtok_in: 0.23
        cost_per_mtok_out: 3.00
        context_length: 262144
      - id: "minimax/minimax-m2.5"
        cost_per_mtok_in: 0.30
        cost_per_mtok_out: 1.10
        context_length: 197000
      - id: "z-ai/glm-5"
        cost_per_mtok_in: 0.30
        cost_per_mtok_out: 2.55
        context_length: 205000
      - id: "google/gemini-3-pro-preview"
        cost_per_mtok_in: 2.00
        cost_per_mtok_out: 12.00
        context_length: 1050000
      - id: "google/gemini-3-flash-preview"
        cost_per_mtok_in: 0.50
        cost_per_mtok_out: 3.00
        context_length: 1050000

  # ── TIER 1b: Cline Extension (current IDE agent) ──────────────────────────
  - id: cline_extension
    name: "Cline VSCodium Extension"
    tier: "1b"
    cost: free  # limited-time special tier — may expire
    requires_auth: true
    auth_type: "anthropic_api_key_via_cline"
    note: "User is on a limited-time special free tier for claude-sonnet-4-6. NOT standard paid API."
    contingency: "If free tier expires, fall back to OpenCode + antigravity-claude-sonnet-4-6"
    sovereignty: partial
    observed_context_behavior:
      advertised_model_limit: 200000
      observed_before_compaction: 250800  # user-observed
      estimated_actual_limit: 400000      # implied by progress bar position
      compaction_note: "May have shadow 400K window via extended context beta"
    models:
      - id: "claude-sonnet-4-6"
        name: "Claude Sonnet 4.6"
        context_length: 200000  # advertised; actual may be 400K
        max_output: 64000
        training_cutoff: "2026-01"
        cost_per_mtok_in: 0  # free tier
        best_for: [ide_integrated_development, code_editing, file_management]

  # ── TIER 6: Local / Sovereign (llama-cpp-python) ──────────────────────────
  - id: local_llama
    name: "llama-cpp-python (Local Sovereign)"
    tier: 6
    cost: free
    requires_auth: false
    sovereignty: full  # zero network, zero telemetry
    api_url: "http://localhost:8080/v1"
    opencode_id: "llama-cpp/local"
    engine: "llama-cpp-python"
    gpu_backend: "Vulkan (Ryzen RDNA2 iGPU)"
    protocol: "expert-knowledge/protocols/LLAMA-CPP-PYTHON-SERVICE-PROTOCOL.md"
    recommended_models:
      - name: "Qwen 2.5 7B Q4"
        vram_gb: 4.4
        context_length: 32768
        speed_toks: "8-20"
        best_for: [code_generation, multilingual, fast]
      - name: "Phi-3.5-mini Q4"
        vram_gb: 2.2
        context_length: 131072
        speed_toks: "20-40"
        best_for: [smallest_footprint, quality_per_param]
      - name: "DeepSeek-R1-Distill-Qwen-7B Q4"
        vram_gb: 4.5
        context_length: 32768
        best_for: [reasoning_tasks]
      - name: "Llama 3.1 8B Q4"
        vram_gb: 4.7
        context_length: 131072
        best_for: [instruction_following, general]
    best_for: [airgap, production, zero_telemetry, offline_dev]

# ─── TASK ROUTING ─────────────────────────────────────────────────────────────
# Maps task types to preferred model choices.
# Format: {task_type: {primary, fallback_chain, min_context, max_context}}

task_routing:

  full_codebase_audit:
    description: "Load and analyze entire repository"
    primary: {provider: opencode_antigravity, model: "google/antigravity-gemini-3-pro"}
    fallback:
      - {provider: gemini_cli, model: "gemini-2.5-pro"}
      - {provider: copilot_cli, model: "gemini-3-flash-preview"}
    min_context: 200000
    cli_flags: "--variant=high"

  architecture_decisions:
    description: "High-stakes design and architecture work"
    primary: {provider: opencode_antigravity, model: "google/antigravity-claude-opus-4-6-thinking"}
    fallback:
      - {provider: opencode_antigravity, model: "google/antigravity-claude-opus-4-5-thinking"}
      - {provider: opencode_builtin, model: "opencode/big-pickle"}
    cli_flags: "--variant=max"

  daily_coding:
    description: "Standard code editing, bug fixes, refactoring"
    primary: {provider: cline_extension, model: "claude-sonnet-4-6"}
    fallback:
      - {provider: opencode_antigravity, model: "google/antigravity-claude-sonnet-4-6"}
      - {provider: opencode_antigravity, model: "google/antigravity-claude-sonnet-4-5"}
      - {provider: opencode_builtin, model: "opencode/big-pickle"}

  fast_prototyping:
    description: "Quick code generation, simple completions"
    primary: {provider: opencode_builtin, model: "opencode/minimax-m2.5-free"}
    fallback:
      - {provider: copilot_cli, model: "claude-haiku-4.5"}
      - {provider: opencode_builtin, model: "opencode/kimi-k2.5-free"}

  research_synthesis:
    description: "Research tasks, document synthesis, web grounded"
    primary: {provider: gemini_cli, model: "gemini-2.5-pro"}
    fallback:
      - {provider: opencode_antigravity, model: "google/antigravity-gemini-3-pro"}
      - {provider: opencode_builtin, model: "opencode/kimi-k2.5-free"}

  github_workflow:
    description: "GitHub-native: issues, PRs, repo navigation"
    primary: {provider: copilot_cli, model: "claude-haiku-4.5"}
    fallback:
      - {provider: copilot_cli, model: "gpt-5-mini"}

  multi_agent_orchestration:
    description: "Sovereign MC agent dispatching tasks"
    primary: {provider: opencode_antigravity, model: "google/antigravity-claude-sonnet-4-6"}
    fallback:
      - {provider: opencode_builtin, model: "opencode/big-pickle"}
      - {provider: local_llama, model: "Qwen 2.5 7B Q4"}
    note: "Dispatched via OpenCodeDispatcher.run_task() in sovereign_mc_agent.py"

  airgap_production:
    description: "Air-gap compliant, zero-telemetry production tasks"
    primary: {provider: local_llama, model: "Qwen 2.5 7B Q4"}
    fallback:
      - {provider: local_llama, model: "Phi-3.5-mini Q4"}
    sovereignty_required: true

  multilingual:
    description: "Chinese, multilingual, or structured logical tasks"
    primary: {provider: opencode_builtin, model: "opencode/glm-5-free"}
    fallback:
      - {provider: openrouter, model: "z-ai/glm-5"}

  context_limit_fallback:
    description: "When primary model context fills up"
    primary: {provider: opencode_antigravity, model: "google/antigravity-gemini-3-flash"}
    fallback:
      - {provider: gemini_cli, model: "gemini-2.5-flash"}
      - {provider: copilot_cli, model: "gemini-3-flash-preview"}
    note: "Reset to 1M context window"

# ─── CONTEXT WINDOW DECISION TREE ──────────────────────────────────────────────
context_routing:
  lt_8k:     {preferred: "opencode/minimax-m2.5-free", reason: "fastest, free"}
  8k_to_50k: {preferred: "google/antigravity-claude-sonnet-4-6", provider: opencode_antigravity, reason: "best quality/speed"}
  50k_to_200k: {preferred: "google/antigravity-claude-opus-4-6-thinking", provider: opencode_antigravity, reason: "extended thinking"}
  200k_to_1M: {preferred: "google/antigravity-gemini-3-pro", provider: opencode_antigravity, reason: "1M context"}
  gt_1M:     {strategy: "chunk_and_summarize", tool: "app/XNAi_rag_app/ingest_library.py"}

# ─── RATE LIMIT FALLBACK WATERFALL ─────────────────────────────────────────────
rate_limit_waterfall:
  - tier: 1
    provider: opencode_antigravity
    on_exhausted: "rotate 3 GitHub accounts (auto)"
    then_fall_to: tier_2
  - tier: 2
    provider: gemini_cli
    quota: "25/day"
    then_fall_to: tier_3
  - tier: 3
    provider: copilot_cli
    quota: "~50 messages/day"
    then_fall_to: tier_4
  - tier: 4
    provider: opencode_builtin
    fallback_order: [big-pickle, kimi-k2.5-free, gpt-5-nano, minimax-m2.5-free, glm-5-free]
    then_fall_to: tier_6
  - tier: 6
    provider: local_llama
    quota: "unlimited"
    sovereignty: full

# ─── SOVEREIGN MC AGENT MODEL ROUTING ──────────────────────────────────────────
# Used by app/XNAi_rag_app/core/sovereign_mc_agent.py OpenCodeDispatcher.MODEL_ROUTING
sovereign_mc_routing:
  architecture: "google/antigravity-claude-opus-4-6-thinking"
  full_codebase: "google/antigravity-gemini-3-pro"
  general: "google/antigravity-claude-sonnet-4-6"
  fast: "opencode/minimax-m2.5-free"
  offline: "llama-cpp/local"
  research: "google/antigravity-gemini-3-pro"
  github: "claude-haiku-4.5"  # via Copilot CLI

# ─── CONFIRMED REAL MODELS REGISTRY ─────────────────────────────────────────
# Used by model_intelligence_ingestion.py and as agent reference.
# GLM-5 is confirmed current name (NOT GLM-4) as of Feb 2026.
confirmed_real_models:
  - id: "moonshotai/kimi-k2.5"
    provider: "Moonshot AI"
    confirmed_date: "2026-02-18"
    source: "HuggingFace + OpenRouter"
    swe_bench: 76.8
    context_k: 256
    architecture: "1T MoE, 32B active"
    multimodal: true
  - id: "minimax/minimax-m2.5"
    provider: "MiniMax"
    confirmed_date: "2026-02-18"
    swe_bench: 80.2
    context_k: 197
  - id: "z-ai/glm-5"
    provider: "Z.ai (Zhipu AI)"
    confirmed_date: "2026-02-18"
    note: "GLM-5 is confirmed current name — NOT GLM-4"
    context_k: 205
  - id: "google/gemini-3-pro-preview"
    provider: "Google"
    confirmed_date: "2026-02-18"
    note: "PUBLIC on OpenRouter as of Feb 2026 — NOT Antigravity-internal only"
    context_k: 1050
  - id: "google/gemini-3-flash-preview"
    provider: "Google"
    confirmed_date: "2026-02-18"
    note: "PUBLIC on OpenRouter as of Feb 2026"
    context_k: 1050
  - id: "anthropic/claude-sonnet-4-6"
    provider: "Anthropic"
    confirmed_date: "2026-02-18"
    training_cutoff: "2026-01"
    context_k: 200
  - id: "anthropic/claude-opus-4-6"
    provider: "Anthropic"
    confirmed_date: "2026-02-18"
    max_output_k: 128
    context_k: 200
