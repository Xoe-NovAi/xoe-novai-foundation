#!/usr/bin/env python3
# ============================================================================
# Xoe-NovAi Phase 1 v0.1.4-stable - Library Ingestion System (ENTERPRISE-GRADE)
# ============================================================================
# Purpose: Enterprise-grade content ingestion from APIs, RSS feeds, and local sources
# Guide Reference: Section 4.4 (Library Ingestion Pipeline)
# Last Updated: 2026-01-08 (Complete Enterprise Implementation)
#
# Features:
#   - Multi-source content ingestion (APIs, RSS, local files)
#   - Enterprise-grade error handling and recovery
#   - Dewey Decimal classification and domain categorization
#   - Batch processing with progress tracking
#   - FAISS vectorstore integration
#   - Comprehensive logging and metrics
#   - Duplicate detection and deduplication
#   - Content quality validation
#   - Rate limiting and backoff strategies
# ============================================================================

import os
import sys
import json
import time
import logging
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
import re
import urllib.parse
from collections import defaultdict, Counter

# Third-party imports (with graceful fallbacks for from_library mode)
try:
    import feedparser
    _HAS_FEEDPARSER = True
except ImportError:
    feedparser = None
    _HAS_FEEDPARSER = False

try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    _HAS_REQUESTS = True
except ImportError:
    requests = None
    HTTPAdapter = None
    Retry = None
    _HAS_REQUESTS = False

try:
    import psutil
    _HAS_PSUTIL = True
except ImportError:
    psutil = None
    _HAS_PSUTIL = False

try:
    import magic  # python-magic for file type detection
    _HAS_MAGIC = True
except ImportError:
    magic = None
    _HAS_MAGIC = False

# Local imports
from XNAi_rag_app.core.config_loader import load_config, get_config_value
from XNAi_rag_app.core.dependencies import get_embeddings, get_vectorstore, get_redis_client
from library_api_integrations import LibraryEnrichmentEngine, DomainCategory
from logging_config import setup_logging, get_logger
from metrics import PerformanceLogger

# Setup logging
setup_logging()
logger = get_logger(__name__)
perf_logger = PerformanceLogger(logger)

# Load configuration
CONFIG = load_config()

# ============================================================================
# DATA STRUCTURES
# ============================================================================

@dataclass
class ScholarlyMetadata:
    """Enhanced metadata for scholarly and classical texts."""
    # Academic identifiers
    isbn: Optional[str] = None
    doi: Optional[str] = None
    issn: Optional[str] = None
    pmid: Optional[str] = None  # PubMed ID
    arxiv_id: Optional[str] = None

    # Classical/Scholarly specific
    era: Optional[str] = None  # e.g., "Ancient Greek", "Medieval", "Renaissance"
    genre: Optional[str] = None  # e.g., "Epic Poetry", "Philosophy", "Drama"
    language_original: Optional[str] = None  # Original language (e.g., "grc", "la")
    translator: Optional[str] = None
    edition: Optional[str] = None  # e.g., "Oxford Classical Texts", "Loeb Edition"
    publication_year: Optional[int] = None
    publisher: Optional[str] = None

    # Scholarly relationships
    related_works: List[str] = field(default_factory=list)  # Related texts/titles
    commentaries: List[str] = field(default_factory=list)   # Known commentaries
    influences: List[str] = field(default_factory=list)    # Influential works
    influenced_by: List[str] = field(default_factory=list) # Influenced works

    # Academic classification
    library_of_congress: Optional[str] = None  # LC classification
    academic_discipline: Optional[str] = None  # e.g., "Classics", "Philosophy"
    subfield: Optional[str] = None  # e.g., "Greek Tragedy", "Platonic Philosophy"

    # Text characteristics
    text_type: Optional[str] = None  # "primary", "secondary", "commentary", "translation"
    word_count: Optional[int] = None
    page_count: Optional[int] = None
    reading_level: Optional[str] = None  # "undergraduate", "graduate", "scholarly"

    # Authority and quality
    scholarly_rating: float = 0.0  # 0-1 scale based on academic reputation
    peer_reviewed: bool = False
    institution_affiliation: Optional[str] = None  # e.g., "Harvard University Press"

@dataclass
class ContentMetadata:
    """Enhanced metadata for ingested content with scholarly extensions."""
    source: str  # 'api', 'rss', 'local', 'web'
    source_url: Optional[str] = None
    title: Optional[str] = None
    author: Optional[str] = None
    description: Optional[str] = None
    content: Optional[str] = None
    content_type: str = 'text'  # 'text', 'audio', 'video', 'image'
    mime_type: Optional[str] = None
    file_size: Optional[int] = None
    language: str = 'en'
    tags: List[str] = field(default_factory=list)
    domain_category: Optional[str] = None
    dewey_decimal: Optional[str] = None
    confidence_score: float = 0.0
    ingestion_timestamp: Optional[str] = None
    last_modified: Optional[str] = None
    checksum: Optional[str] = None
    quality_score: float = 0.0

    # Scholarly extensions
    scholarly: ScholarlyMetadata = field(default_factory=ScholarlyMetadata)

    # Classical text specific
    is_classical_text: bool = False
    classical_era: Optional[str] = None
    classical_language: Optional[str] = None  # 'grc', 'la', 'heb', etc.
    text_critical_notes: Optional[str] = None
    manuscript_tradition: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            'source': self.source,
            'source_url': self.source_url,
            'title': self.title,
            'author': self.author,
            'description': self.description,
            'content_type': self.content_type,
            'mime_type': self.mime_type,
            'file_size': self.file_size,
            'language': self.language,
            'tags': self.tags,
            'domain_category': self.domain_category,
            'dewey_decimal': self.dewey_decimal,
            'confidence_score': self.confidence_score,
            'ingestion_timestamp': self.ingestion_timestamp,
            'last_modified': self.last_modified,
            'checksum': self.checksum,
            'quality_score': self.quality_score
        }

@dataclass
class IngestionStats:
    """Statistics for ingestion operations."""
    total_processed: int = 0
    total_ingested: int = 0
    total_skipped: int = 0
    total_errors: int = 0
    duplicates_found: int = 0
    api_calls_made: int = 0
    rss_feeds_processed: int = 0
    files_processed: int = 0
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    processing_rate: float = 0.0
    memory_peak_mb: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for reporting."""
        duration = self.end_time - self.start_time if self.end_time and self.start_time else 0
        return {
            'total_processed': self.total_processed,
            'total_ingested': self.total_ingested,
            'total_skipped': self.total_skipped,
            'total_errors': self.total_errors,
            'duplicates_found': self.duplicates_found,
            'api_calls_made': self.api_calls_made,
            'rss_feeds_processed': self.rss_feeds_processed,
            'files_processed': self.files_processed,
            'duration_seconds': duration,
            'processing_rate': self.processing_rate,
            'memory_peak_mb': self.memory_peak_mb,
            'success_rate': (self.total_ingested / max(1, self.total_processed)) * 100
        }

# ============================================================================
# SCHOLARLY TEXT CURATION SYSTEM
# ============================================================================

class ScholarlyTextCurator:
    """
    Specialized curator for classical and scholarly texts.

    Handles:
    - Ancient language detection and normalization
    - Historical era classification
    - Citation network analysis
    - Manuscript tradition tracking
    - Cross-reference linking
    """

    def __init__(self):
        # Classical language mappings
        self.classical_languages = {
            'grc': 'Ancient Greek',
            'la': 'Latin',
            'heb': 'Hebrew',
            'arc': 'Aramaic',
            'cop': 'Coptic',
            'syc': 'Syriac',
            'xcl': 'Classical Armenian',
            'got': 'Gothic',
            'chu': 'Old Church Slavonic'
        }

        # Historical eras
        self.historical_eras = {
            'ancient': ['Ancient Greek', 'Classical', 'Hellenistic'],
            'medieval': ['Medieval', 'Byzantine', 'Islamic Golden Age'],
            'renaissance': ['Renaissance', 'Early Modern'],
            'modern': ['Modern', 'Contemporary']
        }

        # Scholarly authority sources
        self.authority_sources = {
            'oxford': 0.95,
            'cambridge': 0.94,
            'harvard': 0.93,
            'yale': 0.92,
            'princeton': 0.91,
            'london': 0.90,
            'berlin': 0.89,
            'sorbonne': 0.88
        }

        # Classical text patterns
        self.classical_patterns = {
            'greek_philosopher': r'\b(Socrates|Plato|Aristotle|Epicurus|Zeno)\b',
            'roman_author': r'\b(Cicero|Virgil|Horace|Ovid|Tacitus)\b',
            'greek_tragedy': r'\b(Aeschylus|Sophocles|Euripides)\b',
            'homeric_epic': r'\b(Iliad|Odyssey|Homer)\b',
            'biblical_text': r'\b(Gospel|Psalms|Genesis|Exodus)\b'
        }

    def detect_classical_language(self, text: str) -> Optional[str]:
        """Detect classical languages in text."""
        # Greek characters
        if re.search(r'[\u0370-\u03FF\u1F00-\u1FFF]', text):
            return 'grc'

        # Latin characters (extended)
        if re.search(r'\b(et|aut|sed|si|in|ad|per|cum|pro)\b', text.lower()):
            return 'la'

        # Hebrew characters
        if re.search(r'[\u0590-\u05FF]', text):
            return 'heb'

        return None

    def classify_historical_era(self, title: str, author: str, content: str) -> Optional[str]:
        """Classify historical era of text."""
        text_combined = f"{title} {author} {content}".lower()

        # Ancient indicators
        if any(term in text_combined for term in ['ancient', 'classical', 'hellenistic', 'bc', 'bce']):
            return 'ancient'

        # Medieval indicators
        if any(term in text_combined for term in ['medieval', 'byzantine', 'dark ages', 'feudal']):
            return 'medieval'

        # Author-based classification
        ancient_authors = ['plato', 'aristotle', 'socrates', 'homer', 'virgil', 'cicero']
        medieval_authors = ['thomas aquinas', 'dante', 'chaucer']

        if any(author.lower().replace(' ', '') in a for a in ancient_authors):
            return 'ancient'
        if any(author.lower().replace(' ', '') in m for m in medieval_authors):
            return 'medieval'

        return None

    def assess_scholarly_authority(self, publisher: str, source: str) -> float:
        """Assess scholarly authority on 0-1 scale."""
        authority_score = 0.5  # Default

        # Publisher authority
        publisher_lower = publisher.lower() if publisher else ""
        for pub, score in self.authority_sources.items():
            if pub in publisher_lower:
                authority_score = max(authority_score, score)

        # Source authority
        if 'university' in source.lower() or 'press' in source.lower():
            authority_score += 0.1

        return min(1.0, authority_score)

    def build_citation_network(self, metadata: ContentMetadata) -> Dict[str, List[str]]:
        """Build citation network relationships."""
        relationships = {
            'cites': [],
            'cited_by': [],
            'related_works': [],
            'influences': [],
            'commentaries': []
        }

        title = metadata.title or ""
        author = metadata.author or ""
        content = metadata.content or ""

        # Extract citation patterns
        citation_patterns = [
            r'cf\.?\s+([^,\n]{1,100})',  # cf. references
            r'see\s+([^,\n]{1,100})',    # see references
            r'compare\s+([^,\n]{1,100})' # compare references
        ]

        for pattern in citation_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            relationships['cites'].extend(matches[:5])  # Limit to 5

        # Classical text relationships
        if metadata.is_classical_text:
            # Add known commentaries and influences
            classical_relationships = self._get_classical_relationships(title, author)
            relationships.update(classical_relationships)

        return relationships

    def _get_classical_relationships(self, title: str, author: str) -> Dict[str, List[str]]:
        """Get known relationships for classical texts."""
        relationships = defaultdict(list)

        # Plato relationships
        if 'plato' in author.lower():
            if 'republic' in title.lower():
                relationships['commentaries'].extend([
                    'Plato: Republic (Bloom Commentary)',
                    'The Cambridge Companion to Plato\'s Republic'
                ])
                relationships['influences'].extend([
                    'Aristotle', 'Augustine', 'Hobbes', 'Rousseau'
                ])

        # Aristotle relationships
        elif 'aristotle' in author.lower():
            if 'nicomachean ethics' in title.lower():
                relationships['commentaries'].extend([
                    'Aristotle: Nicomachean Ethics (Broadie & Rowe)',
                    'The Cambridge Companion to Aristotle\'s Ethics'
                ])

        # Homer relationships
        elif 'homer' in author.lower():
            relationships['commentaries'].extend([
                'The Iliad (Lattimore translation)',
                'A Commentary on Homer\'s Odyssey'
            ])

        return dict(relationships)

    def normalize_classical_text(self, content: str, language: str) -> str:
        """Normalize classical text (archaic spelling, variant forms)."""
        if language == 'grc':
            # Greek normalization
            content = self._normalize_greek_text(content)
        elif language == 'la':
            # Latin normalization
            content = self._normalize_latin_text(content)

        return content

    def _normalize_greek_text(self, text: str) -> str:
        """Normalize Greek text variants."""
        # Common variant normalizations
        normalizations = {
            'ς': 'σ',  # Final sigma to sigma
            'ϲ': 'σ',  # Old sigma variant
            '᾽': "'",   # Greek apostrophe
        }

        for old, new in normalizations.items():
            text = text.replace(old, new)

        return text

    def _normalize_latin_text(self, text: str) -> str:
        """Normalize Latin text variants."""
        # Common variant normalizations
        normalizations = {
            'v': 'u',  # Medieval v/u confusion (optional, context-dependent)
            'j': 'i',  # j/i confusion (optional, context-dependent)
        }

        # Only apply conservative normalizations
        return text

class DomainKnowledgeBaseConstructor:
    """
    Constructor for domain-specific knowledge bases for LLM experts.

    Builds specialized knowledge bases under knowledge/ directory
    that can be dynamically loaded by LLM experts.
    """

    def __init__(self, domain: str, knowledge_base_path: str = "/knowledge"):
        self.domain = domain
        self.kb_path = Path(knowledge_base_path) / domain
        self.kb_path.mkdir(parents=True, exist_ok=True)

        # Domain-specific configurations
        self.domain_configs = {
            'classics': {
                'languages': ['grc', 'la', 'heb'],
                'eras': ['ancient', 'medieval'],
                'min_authority_score': 0.7,
                'max_texts': 1000
            },
            'philosophy': {
                'languages': ['en', 'de', 'fr', 'grc', 'la'],
                'eras': ['ancient', 'medieval', 'modern'],
                'min_authority_score': 0.8,
                'max_texts': 500
            },
            'literature': {
                'languages': ['en', 'fr', 'de', 'es'],
                'eras': ['renaissance', 'modern'],
                'min_authority_score': 0.6,
                'max_texts': 2000
            }
        }

    def construct_knowledge_base(self, source_texts: List[ContentMetadata]) -> Dict[str, Any]:
        """
        Construct domain-specific knowledge base from source texts.

        Returns knowledge base metadata and stores processed content.
        """
        logger.info(f"Constructing {self.domain} knowledge base with {len(source_texts)} texts")

        # Filter and validate texts for domain
        validated_texts = self._validate_domain_texts(source_texts)

        # Build domain ontology
        ontology = self._build_domain_ontology(validated_texts)

        # Create expert profiles
        expert_profiles = self._create_expert_profiles(validated_texts)

        # Store processed knowledge base
        kb_metadata = {
            'domain': self.domain,
            'total_texts': len(validated_texts),
            'ontology': ontology,
            'expert_profiles': expert_profiles,
            'created_at': datetime.now().isoformat(),
            'quality_metrics': self._calculate_quality_metrics(validated_texts)
        }

        # Save knowledge base
        self._save_knowledge_base(validated_texts, kb_metadata)

        return kb_metadata

    def _validate_domain_texts(self, texts: List[ContentMetadata]) -> List[ContentMetadata]:
        """Validate texts for domain relevance and quality."""
        config = self.domain_configs.get(self.domain, {})
        min_authority = config.get('min_authority_score', 0.5)

        validated = []
        for text in texts:
            # Domain relevance check
            if not self._is_domain_relevant(text):
                continue

            # Authority check
            if hasattr(text, 'scholarly') and text.scholarly:
                if text.scholarly.scholarly_rating < min_authority:
                    continue

            # Quality check
            if text.quality_score < 0.5:
                continue

            validated.append(text)

        return validated[:config.get('max_texts', 1000)]

    def _is_domain_relevant(self, text: ContentMetadata) -> bool:
        """Check if text is relevant to domain."""
        content = f"{text.title} {text.author} {text.content}".lower()

        domain_keywords = {
            'classics': ['ancient', 'greek', 'latin', 'classical', 'antiquity', 'plato', 'aristotle', 'homer'],
            'philosophy': ['philosophy', 'metaphysics', 'ethics', 'epistemology', 'ontology'],
            'literature': ['novel', 'poetry', 'drama', 'fiction', 'literature', 'author']
        }

        keywords = domain_keywords.get(self.domain, [])
        return any(keyword in content for keyword in keywords)

    def _build_domain_ontology(self, texts: List[ContentMetadata]) -> Dict[str, Any]:
        """Build domain ontology from texts."""
        ontology = {
            'concepts': set(),
            'relationships': [],
            'hierarchies': {},
            'authorities': {}
        }

        for text in texts:
            # Extract concepts (simplified - could use NLP)
            if text.tags:
                ontology['concepts'].update(text.tags)

            # Build authority rankings
            if hasattr(text, 'scholarly') and text.scholarly:
                author = text.author or 'Unknown'
                rating = text.scholarly.scholarly_rating
                ontology['authorities'][author] = max(
                    ontology['authorities'].get(author, 0),
                    rating
                )

        # Convert sets to lists for JSON serialization
        ontology['concepts'] = list(ontology['concepts'])

        return ontology

    def _create_expert_profiles(self, texts: List[ContentMetadata]) -> Dict[str, Any]:
        """Create expert profiles for domain."""
        profiles = {}

        # Group texts by key figures/concepts
        if self.domain == 'classics':
            profiles = self._create_classical_expert_profiles(texts)
        elif self.domain == 'philosophy':
            profiles = self._create_philosophy_expert_profiles(texts)

        return profiles

    def _create_classical_expert_profiles(self, texts: List[ContentMetadata]) -> Dict[str, Any]:
        """Create expert profiles for classical studies."""
        profiles = {}

        # Plato expert
        plato_texts = [t for t in texts if 'plato' in (t.author or '').lower()]
        if plato_texts:
            profiles['plato_expert'] = {
                'name': 'Plato Scholar',
                'specialty': 'Platonic Philosophy',
                'texts_count': len(plato_texts),
                'eras': ['Ancient Greek'],
                'key_works': ['Republic', 'Symposium', 'Phaedo'],
                'methodology': 'Dialogic method, Theory of Forms'
            }

        # Aristotle expert
        aristotle_texts = [t for t in texts if 'aristotle' in (t.author or '').lower()]
        if aristotle_texts:
            profiles['aristotle_expert'] = {
                'name': 'Aristotle Scholar',
                'specialty': 'Aristotelian Philosophy',
                'texts_count': len(aristotle_texts),
                'eras': ['Ancient Greek'],
                'key_works': ['Nicomachean Ethics', 'Politics', 'Metaphysics'],
                'methodology': 'Empirical observation, Syllogistic reasoning'
            }

        return profiles

    def _create_philosophy_expert_profiles(self, texts: List[ContentMetadata]) -> Dict[str, Any]:
        """Create expert profiles for philosophy."""
        profiles = {}

        # Kant expert
        kant_texts = [t for t in texts if 'kant' in (t.author or '').lower()]
        if kant_texts:
            profiles['kant_expert'] = {
                'name': 'Kant Scholar',
                'specialty': 'German Idealism',
                'texts_count': len(kant_texts),
                'eras': ['Modern'],
                'key_works': ['Critique of Pure Reason', 'Groundwork of the Metaphysics of Morals'],
                'methodology': 'Transcendental idealism, Categorical imperative'
            }

        return profiles

    def _calculate_quality_metrics(self, texts: List[ContentMetadata]) -> Dict[str, float]:
        """Calculate quality metrics for knowledge base."""
        if not texts:
            return {}

        authority_scores = []
        quality_scores = []

        for text in texts:
            if hasattr(text, 'scholarly') and text.scholarly:
                authority_scores.append(text.scholarly.scholarly_rating)
            quality_scores.append(text.quality_score)

        return {
            'avg_authority_score': sum(authority_scores) / len(authority_scores) if authority_scores else 0,
            'avg_quality_score': sum(quality_scores) / len(quality_scores) if quality_scores else 0,
            'total_texts': len(texts),
            'languages_covered': len(set(t.language for t in texts if t.language)),
            'eras_covered': len(set(t.scholarly.era for t in texts if hasattr(t, 'scholarly') and t.scholarly and t.scholarly.era))
        }

    def _save_knowledge_base(self, texts: List[ContentMetadata], metadata: Dict[str, Any]):
        """Save knowledge base to disk."""
        # Save metadata
        metadata_file = self.kb_path / 'metadata.json'
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        # Save texts (simplified - could use vectorstore)
        texts_file = self.kb_path / 'texts.json'
        texts_data = [text.to_dict() for text in texts]
        with open(texts_file, 'w', encoding='utf-8') as f:
            json.dump(texts_data, f, indent=2, ensure_ascii=False)

        logger.info(f"Saved {self.domain} knowledge base to {self.kb_path}")

# ============================================================================
# ENTERPRISE-GRADE INGESTION ENGINE
# ============================================================================

class EnterpriseIngestionEngine:
    """
    Enterprise-grade content ingestion engine.

    Supports multiple content sources:
    - Library APIs (Open Library, Google Books, etc.)
    - RSS feeds (podcasts, blogs, news)
    - Local files (PDF, TXT, MD, audio)
    - Web content (via crawler integration)
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Initialize the ingestion engine."""
        self.config = config or CONFIG
        self.redis_client = get_redis_client()

        # Initialize components
        self.enrichment_engine = LibraryEnrichmentEngine()
        self.scholarly_curator = ScholarlyTextCurator()
        self.embeddings = None
        self.vectorstore = None

        # Content deduplication
        self.processed_checksums: Set[str] = set()

        # Rate limiting
        self.last_api_call = 0.0
        self.api_call_interval = 1.0  # 1 second between API calls

        # Quality thresholds
        self.min_content_length = 100
        self.min_quality_score = 0.3
        self.max_duplicate_similarity = 0.95

        # Scholarly quality thresholds
        self.min_scholarly_rating = 0.6
        self.classical_text_min_length = 500

        # CPU-optimized processing (AMD Ryzen 7 5700U constraints)
        self.cpu_cores = 6  # Optimized for 6 cores (75% utilization)
        self.memory_limit_gb = 12  # Leave 4GB for system (16GB total)
        self.batch_size_cpu = 50  # Smaller batches for CPU processing
        self.max_concurrent_batches = 3  # Limit concurrent processing

        # Multi-domain processing configurations
        self.domain_configs = {
            'science': {
                'quality_keywords': ['research', 'study', 'analysis', 'experiment', 'hypothesis'],
                'authority_sources': ['nature', 'science', 'cell', 'plos', 'arxiv'],
                'min_quality_score': 0.7,
                'language_priority': ['en', 'de', 'fr']
            },
            'technology': {
                'quality_keywords': ['algorithm', 'framework', 'architecture', 'implementation', 'optimization'],
                'authority_sources': ['ieee', 'acm', 'mit', 'stanford', 'berkeley'],
                'min_quality_score': 0.6,
                'language_priority': ['en', 'zh', 'ja']
            },
            'occult': {
                'quality_keywords': ['esoteric', 'mystical', 'occult', 'spiritual', 'metaphysical'],
                'authority_sources': ['hermetic', 'theosophical', 'rosicrucian', 'golden_dawn'],
                'min_quality_score': 0.4,  # Lower threshold for diverse sources
                'language_priority': ['en', 'la', 'grc', 'ar']
            },
            'spiritual': {
                'quality_keywords': ['meditation', 'consciousness', 'enlightenment', 'spiritual', 'mindfulness'],
                'authority_sources': ['buddhist', 'hindu', 'taoist', 'sufi', 'mystical'],
                'min_quality_score': 0.5,
                'language_priority': ['en', 'sa', 'zh', 'ar', 'ti']
            },
            'astrology': {
                'quality_keywords': ['natal', 'transit', 'horoscope', 'astral', 'zodiac', 'planetary'],
                'authority_sources': ['astrological', 'astronomical', 'vedic', 'western'],
                'min_quality_score': 0.4,
                'language_priority': ['en', 'la', 'sa']
            },
            'esoteric': {
                'quality_keywords': ['esoteric', 'secret', 'hidden', 'mysterious', 'arcane', 'occult'],
                'authority_sources': ['esoteric', 'hermetic', 'alchemical', 'kabbalistic'],
                'min_quality_score': 0.4,
                'language_priority': ['en', 'la', 'grc', 'heb', 'ar']
            },
            'science_fiction': {
                'quality_keywords': ['sci-fi', 'speculative', 'futuristic', 'cyberpunk', 'space opera'],
                'authority_sources': ['hugo_award', 'nebula_award', 'literary', 'academic'],
                'min_quality_score': 0.5,
                'language_priority': ['en', 'fr', 'de', 'ru', 'jp']
            },
            'youtube': {
                'quality_keywords': ['video', 'transcript', 'lecture', 'interview', 'discussion'],
                'authority_sources': ['university', 'expert', 'academic', 'professional'],
                'min_quality_score': 0.6,
                'content_types': ['transcript', 'caption', 'description']
            }
        }

        # File processing
        self.supported_extensions = {
            'text': ['.txt', '.md', '.rst', '.html', '.xml'],
            'document': ['.pdf', '.doc', '.docx', '.epub'],
            'audio': ['.mp3', '.wav', '.flac', '.ogg', '.m4a'],
            'video': ['.mp4', '.avi', '.mkv', '.webm'],
            'image': ['.jpg', '.jpeg', '.png', '.gif', '.webp']
        }

        logger.info("Enterprise Ingestion Engine initialized with scholarly curation")

    def _rate_limit_api_call(self):
        """Apply rate limiting to API calls."""
        now = time.time()
        time_since_last = now - self.last_api_call

        if time_since_last < self.api_call_interval:
            sleep_time = self.api_call_interval - time_since_last
            time.sleep(sleep_time)

        self.last_api_call = time.time()

    def _calculate_checksum(self, content: str) -> str:
        """Calculate SHA256 checksum of content."""
        return hashlib.sha256(content.encode('utf-8')).hexdigest()

    def _is_duplicate(self, checksum: str) -> bool:
        """Check if content has already been processed."""
        return checksum in self.processed_checksums

    def _assess_content_quality(self, content: str, metadata: ContentMetadata) -> float:
        """
        Assess content quality on a 0-1 scale.

        Factors:
        - Length and readability
        - Presence of metadata
        - Source reliability
        - Language detection
        - Content coherence
        """
        score = 0.0

        # Length factor (0-0.3)
        content_length = len(content.strip())
        if content_length >= self.min_content_length:
            length_score = min(0.3, content_length / 2000)  # Max at 2000 chars
            score += length_score

        # Metadata completeness (0-0.3)
        metadata_fields = [metadata.title, metadata.author, metadata.description]
        filled_fields = sum(1 for field in metadata_fields if field and field.strip())
        metadata_score = (filled_fields / len(metadata_fields)) * 0.3
        score += metadata_score

        # Source reliability (0-0.2)
        reliable_sources = ['gutenberg', 'google', 'openlibrary', 'arxiv', 'pubmed']
        if any(source in metadata.source.lower() for source in reliable_sources):
            score += 0.2

        # Language factor (0-0.1)
        if metadata.language == 'en':
            score += 0.1

        # Content coherence (0-0.1)
        # Basic heuristic: ratio of alphanumeric to total characters
        if content_length > 0:
            alpha_ratio = len(re.findall(r'[a-zA-Z0-9]', content)) / content_length
            coherence_score = min(0.1, alpha_ratio * 0.1)
            score += coherence_score

        return min(1.0, score)

    def _enrich_metadata(self, metadata: ContentMetadata) -> ContentMetadata:
        """
        Enrich content metadata using library APIs.

        Applies domain classification and Dewey Decimal mapping.
        """
        try:
            # Prepare enrichment data
            enrichment_data = {
                'title': metadata.title or 'Unknown Title',
                'content': metadata.content or metadata.description or '',
                'author': metadata.author
            }

            # Perform enrichment
            result = self.enrichment_engine.classify_and_enrich(
                title=enrichment_data['title'],
                content=enrichment_data['content'],
                author=enrichment_data['author']
            )

            # Update metadata
            if result.get('domain_category'):
                metadata.domain_category = result['domain_category'].value
                metadata.confidence_score = result.get('category_confidence', 0.0)

            if result.get('primary_dewey'):
                metadata.dewey_decimal = result['primary_dewey']

            # Add enrichment tags
            if result.get('metadata_results'):
                for meta_result in result['metadata_results'][:3]:  # Top 3 results
                    if meta_result.get('subjects'):
                        metadata.tags.extend(meta_result['subjects'][:5])  # Top 5 subjects

            # Remove duplicates from tags
            metadata.tags = list(set(metadata.tags))

        except Exception as e:
            logger.warning(f"Metadata enrichment failed: {e}")

        return metadata

    def _apply_scholarly_enhancements(self, metadata: ContentMetadata) -> ContentMetadata:
        """
        Apply scholarly enhancements to metadata.

        Detects classical texts, applies scholarly enrichment, and builds citation networks.
        """
        try:
            # Detect classical text characteristics
            is_classical = self._detect_classical_text(metadata)
            metadata.is_classical_text = is_classical

            if is_classical:
                # Detect classical language
                classical_lang = self.scholarly_curator.detect_classical_language(
                    metadata.content or metadata.title or ""
                )
                metadata.classical_language = classical_lang

                # Classify historical era
                era = self.scholarly_curator.classify_historical_era(
                    metadata.title or "",
                    metadata.author or "",
                    metadata.content or ""
                )
                metadata.classical_era = era

                # Normalize text if classical
                if classical_lang and metadata.content:
                    metadata.content = self.scholarly_curator.normalize_classical_text(
                        metadata.content, classical_lang
                    )

            # Assess scholarly authority
            scholarly_rating = self.scholarly_curator.assess_scholarly_authority(
                metadata.scholarly.publisher or "",
                metadata.source
            )
            metadata.scholarly.scholarly_rating = scholarly_rating

            # Build citation network
            citation_network = self.scholarly_curator.build_citation_network(metadata)

            # Update scholarly metadata
            metadata.scholarly.era = metadata.classical_era
            metadata.scholarly.language_original = metadata.classical_language
            metadata.scholarly.academic_discipline = self._classify_academic_discipline(metadata)
            metadata.scholarly.related_works = citation_network.get('related_works', [])
            metadata.scholarly.influences = citation_network.get('influences', [])
            metadata.scholarly.commentaries = citation_network.get('commentaries', [])

            # Word count for scholarly texts
            if metadata.content:
                metadata.scholarly.word_count = len(metadata.content.split())

        except Exception as e:
            logger.warning(f"Scholarly enhancement failed: {e}")

        return metadata

    def _detect_classical_text(self, metadata: ContentMetadata) -> bool:
        """Detect if content is classical/scholarly text."""
        text_combined = f"{metadata.title} {metadata.author} {metadata.content}".lower()

        # Check for classical patterns
        for pattern_name, pattern in self.scholarly_curator.classical_patterns.items():
            if re.search(pattern, text_combined, re.IGNORECASE):
                return True

        # Check for scholarly indicators
        scholarly_indicators = [
            'ancient', 'classical', 'antiquity', 'medieval', 'renaissance',
            'manuscript', 'codex', 'scroll', 'tablet', 'inscription',
            'bce', 'bc', 'ce', 'ad', 'century', 'millennium',
            'aristotle', 'plato', 'socrates', 'homer', 'virgil', 'cicero',
            'oxford classical texts', 'loeb classical library',
            'cambridge greek and latin classics'
        ]

        indicator_count = sum(1 for indicator in scholarly_indicators if indicator in text_combined)
        return indicator_count >= 2  # At least 2 scholarly indicators

    def _classify_academic_discipline(self, metadata: ContentMetadata) -> Optional[str]:
        """Classify academic discipline."""
        text_combined = f"{metadata.title} {metadata.author} {metadata.content}".lower()

        if any(term in text_combined for term in ['philosophy', 'metaphysics', 'ethics', 'logic']):
            return 'Philosophy'
        elif any(term in text_combined for term in ['history', 'chronicle', 'annals']):
            return 'History'
        elif any(term in text_combined for term in ['literature', 'poetry', 'drama', 'epic']):
            return 'Literature'
        elif any(term in text_combined for term in ['theology', 'religion', 'divine']):
            return 'Religious Studies'
        elif any(term in text_combined for term in ['law', 'justice', 'rights']):
            return 'Law'
        elif any(term in text_combined for term in ['science', 'nature', 'elements']):
            return 'Natural Philosophy'

        return 'Classics'  # Default for classical texts

    def _store_in_vectorstore(self, content: str, metadata: ContentMetadata) -> bool:
        """
        Store content in FAISS vectorstore.

        Returns True if successfully stored.
        """
        try:
            # Lazy initialization
            if self.embeddings is None:
                self.embeddings = get_embeddings()

            if self.vectorstore is None:
                self.vectorstore = get_vectorstore(embeddings=self.embeddings)

            if self.vectorstore is None:
                logger.error("Vectorstore not available")
                return False

            # Create document
            doc_metadata = metadata.to_dict()
            doc_metadata['ingestion_timestamp'] = datetime.now().isoformat()

            from langchain_core.documents import Document
            doc = Document(
                page_content=content,
                metadata=doc_metadata
            )

            # Add to vectorstore
            self.vectorstore.add_documents([doc])

            logger.info(f"Stored document: {metadata.title} ({len(content)} chars)")
            return True

        except Exception as e:
            logger.error(f"Vectorstore storage failed: {e}")
            return False

def _build_truncated_context(self, content: str, max_length: int = 2000) -> str:
    """
    Build truncated context for content processing.

    Args:
        content: Original content to truncate
        max_length: Maximum length of truncated content

    Returns:
        Truncated content with context preservation
    """
    if len(content) <= max_length:
        return content

    # Preserve context by keeping beginning and end
    # Keep first 1000 chars and last 1000 chars with ellipsis
    return content[:1000] + " ... [truncated] ... " + content[-1000:]

def _cache_processed_checksum(self, checksum: str):
    """Cache checksum of processed content."""
    self.processed_checksums.add(checksum)

    # Also store in Redis for persistence
    try:
        cache_key = f"ingestion:checksum:{checksum}"
        self.redis_client.setex(cache_key, 86400 * 30, "1")  # 30 days
    except Exception as e:
        logger.warning(f"Redis checksum caching failed: {e}")

    def ingest_from_api(self, api_name: str, query: str, max_items: int = 50) -> IngestionStats:
        """
        Ingest content from library APIs.

        Supported APIs: openlibrary, google_books, internet_archive, gutenberg
        """
        stats = IngestionStats()
        stats.start_time = time.time()

        try:
            logger.info(f"Starting API ingestion: {api_name} with query '{query}'")

            # Get API client
            api_client = self.enrichment_engine.get_api_client(api_name)
            if not api_client:
                raise ValueError(f"Unknown API: {api_name}")

            # Search API
            self._rate_limit_api_call()
            results = api_client.search(query, max_results=max_items)
            stats.api_calls_made += 1

            logger.info(f"API returned {len(results)} results")

            # Process results
            for result in results:
                stats.total_processed += 1

                try:
                    # Create metadata
                    metadata = ContentMetadata(
                        source=f'api_{api_name}',
                        source_url=result.get('url') or result.get('link'),
                        title=result.get('title'),
                        author=result.get('author') or result.get('authors'),
                        description=result.get('description'),
                        content=result.get('content') or result.get('text'),
                        content_type='text',
                        language=result.get('language', 'en'),
                        tags=result.get('subjects', []),
                        ingestion_timestamp=datetime.now().isoformat()
                    )

                    # Calculate checksum
                    content_for_checksum = metadata.content or metadata.title or ""
                    checksum = self._calculate_checksum(content_for_checksum)

                    # Check for duplicates
                    if self._is_duplicate(checksum):
                        stats.duplicates_found += 1
                        stats.total_skipped += 1
                        continue

                    # Assess quality
                    if metadata.content:
                        metadata.quality_score = self._assess_content_quality(metadata.content, metadata)

                        if metadata.quality_score < self.min_quality_score:
                            logger.debug(f"Skipping low-quality content: {metadata.title} (score: {metadata.quality_score:.2f})")
                            stats.total_skipped += 1
                            continue

                    # Enrich metadata
                    metadata = self._enrich_metadata(metadata)

                    # Apply scholarly enhancements
                    metadata = self._apply_scholarly_enhancements(metadata)

                    # Store in vectorstore
                    if metadata.content and self._store_in_vectorstore(metadata.content, metadata):
                        stats.total_ingested += 1
                        self._cache_processed_checksum(checksum)
                    else:
                        stats.total_errors += 1

                except Exception as e:
                    logger.error(f"Error processing API result: {e}")
                    stats.total_errors += 1

        except Exception as e:
            logger.error(f"API ingestion failed: {e}")
            stats.total_errors += 1

        stats.end_time = time.time()
        if stats.end_time and stats.start_time:
            duration = stats.end_time - stats.start_time
            stats.processing_rate = stats.total_processed / duration if duration > 0 else 0

        return stats

    def ingest_from_rss(self, rss_urls: List[str]) -> IngestionStats:
        """
        Ingest content from RSS feeds.

        Supports podcasts, blogs, news feeds, etc.
        """
        stats = IngestionStats()
        stats.start_time = time.time()

        try:
            logger.info(f"Starting RSS ingestion from {len(rss_urls)} feeds")

            for rss_url in rss_urls:
                stats.rss_feeds_processed += 1
                logger.info(f"Processing RSS feed: {rss_url}")

                try:
                    # Parse RSS feed
                    feed = feedparser.parse(rss_url)

                    if feed.get('status') != 200:
                        logger.warning(f"RSS feed returned status {feed.get('status')}: {rss_url}")
                        continue

                    logger.info(f"RSS feed contains {len(feed.entries)} entries")

                    # Process entries
                    for entry in feed.entries[:50]:  # Limit to 50 per feed
                        stats.total_processed += 1

                        try:
                            # Extract content
                            content = ""
                            if hasattr(entry, 'content') and entry.content:
                                content = entry.content[0].value if isinstance(entry.content, list) else entry.content
                            elif hasattr(entry, 'summary'):
                                content = entry.summary
                            elif hasattr(entry, 'description'):
                                content = entry.description

                            # Clean HTML tags
                            content = re.sub(r'<[^>]+>', '', content)

                            # Create metadata
                            metadata = ContentMetadata(
                                source='rss',
                                source_url=getattr(entry, 'link', ''),
                                title=getattr(entry, 'title', ''),
                                author=getattr(entry, 'author', ''),
                                description=getattr(entry, 'summary', ''),
                                content=content,
                                content_type='text',
                                language='en',  # Assume English, could be detected
                                tags=getattr(entry, 'tags', []),
                                ingestion_timestamp=datetime.now().isoformat(),
                                last_modified=getattr(entry, 'updated', None)
                            )

                            # Calculate checksum
                            content_for_checksum = metadata.content or metadata.title or ""
                            checksum = self._calculate_checksum(content_for_checksum)

                            # Check for duplicates
                            if self._is_duplicate(checksum):
                                stats.duplicates_found += 1
                                stats.total_skipped += 1
                                continue

                            # Assess quality
                            if metadata.content and len(metadata.content.strip()) >= self.min_content_length:
                                metadata.quality_score = self._assess_content_quality(metadata.content, metadata)

                                if metadata.quality_score < self.min_quality_score:
                                    stats.total_skipped += 1
                                    continue

                                # Enrich metadata
                                metadata = self._enrich_metadata(metadata)

                                # Store in vectorstore
                                if self._store_in_vectorstore(metadata.content, metadata):
                                    stats.total_ingested += 1
                                    self._cache_processed_checksum(checksum)
                                else:
                                    stats.total_errors += 1
                            else:
                                stats.total_skipped += 1

                        except Exception as e:
                            logger.error(f"Error processing RSS entry: {e}")
                            stats.total_errors += 1

                except Exception as e:
                    logger.error(f"Error processing RSS feed {rss_url}: {e}")
                    stats.total_errors += 1

        except Exception as e:
            logger.error(f"RSS ingestion failed: {e}")
            stats.total_errors += 1

        stats.end_time = time.time()
        if stats.end_time and stats.start_time:
            duration = stats.end_time - stats.start_time
            stats.processing_rate = stats.total_processed / duration if duration > 0 else 0

        return stats

    def ingest_from_directory(self, directory_path: str, recursive: bool = True) -> IngestionStats:
        """
        Ingest content from local directory.

        Supports text files, PDFs, audio files, etc.
        """
        stats = IngestionStats()
        stats.start_time = time.time()

        try:
            directory = Path(directory_path)
            if not directory.exists():
                raise FileNotFoundError(f"Directory not found: {directory_path}")

            logger.info(f"Starting directory ingestion: {directory_path}")

            # Find files
            pattern = "**/*" if recursive else "*"
            files = []
            for ext_list in self.supported_extensions.values():
                for ext in ext_list:
                    files.extend(directory.glob(f"{pattern}{ext}"))

            files = list(set(files))  # Remove duplicates
            logger.info(f"Found {len(files)} files to process")

            # Process files
            for file_path in files:
                stats.files_processed += 1
                stats.total_processed += 1

                try:
                    # Get file info
                    file_stat = file_path.stat()
                    file_size = file_stat.st_size

                    # Detect MIME type
                    mime_type = magic.from_file(str(file_path), mime=True)

                    # Determine content type
                    content_type = 'text'  # Default
                    for type_name, extensions in self.supported_extensions.items():
                        if file_path.suffix.lower() in extensions:
                            content_type = type_name
                            break

                    # Read content based on type
                    content = ""
                    if content_type == 'text':
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                        except UnicodeDecodeError:
                            # Try with errors='ignore'
                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                content = f.read()
                    else:
                        # For binary files, just store metadata
                        content = f"[Binary file: {file_path.name}]"

                    # Create metadata
                    metadata = ContentMetadata(
                        source='local_file',
                        source_url=str(file_path),
                        title=file_path.stem,
                        content=content,
                        content_type=content_type,
                        mime_type=mime_type,
                        file_size=file_size,
                        language='en',  # Could be detected
                        ingestion_timestamp=datetime.now().isoformat(),
                        last_modified=datetime.fromtimestamp(file_stat.st_mtime).isoformat()
                    )

                    # Calculate checksum
                    content_for_checksum = content + str(file_path)
                    checksum = self._calculate_checksum(content_for_checksum)

                    # Check for duplicates
                    if self._is_duplicate(checksum):
                        stats.duplicates_found += 1
                        stats.total_skipped += 1
                        continue

                    # Assess quality for text content
                    if content_type == 'text' and content:
                        metadata.quality_score = self._assess_content_quality(content, metadata)

                        if metadata.quality_score < self.min_quality_score:
                            stats.total_skipped += 1
                            continue

                        # Enrich metadata
                        metadata = self._enrich_metadata(metadata)

                    # Store in vectorstore
                    if self._store_in_vectorstore(content, metadata):
                        stats.total_ingested += 1
                        self._cache_processed_checksum(checksum)
                        logger.info(f"Ingested file: {file_path.name} ({content_type}, {file_size} bytes)")
                    else:
                        stats.total_errors += 1

                except Exception as e:
                    logger.error(f"Error processing file {file_path}: {e}")
                    stats.total_errors += 1

        except Exception as e:
            logger.error(f"Directory ingestion failed: {e}")
            stats.total_errors += 1

        stats.end_time = time.time()
        if stats.end_time and stats.start_time:
            duration = stats.end_time - stats.start_time
            stats.processing_rate = stats.total_processed / duration if duration > 0 else 0

        return stats

# ============================================================================
# PUBLIC API FUNCTIONS
# ============================================================================

def ingest_library(
    library_path: Optional[str] = None,
    batch_size: int = 100,
    max_items: int = 1000,
    sources: Optional[List[str]] = None,
    enable_deduplication: bool = True,
    enable_quality_filter: bool = True
) -> Tuple[int, float]:
    """
    Main ingestion function - enterprise-grade library content ingestion.

    Args:
        library_path: Path to library directory (default: from config)
        batch_size: Processing batch size
        max_items: Maximum items to process
        sources: List of sources to ingest from ['api', 'rss', 'local']
        enable_deduplication: Enable duplicate detection
        enable_quality_filter: Enable quality filtering

    Returns:
        Tuple of (total_ingested, processing_time_seconds)
    """
    start_time = time.time()

    # Initialize engine
    engine = EnterpriseIngestionEngine()

    # Default sources
    if sources is None:
        sources = ['api', 'rss', 'local']

    # Get library path
    if library_path is None:
        library_path = get_config_value("files.library_path", "/library")

    total_ingested = 0
    all_stats = []

    logger.info("=" * 80)
    logger.info("XOE-NOVAI ENTERPRISE LIBRARY INGESTION")
    logger.info("=" * 80)
    logger.info(f"Sources: {', '.join(sources)}")
    logger.info(f"Library path: {library_path}")
    logger.info(f"Batch size: {batch_size}")
    logger.info(f"Max items: {max_items}")

    try:
        # API Ingestion
        if 'api' in sources:
            logger.info("\n" + "="*50)
            logger.info("API INGESTION PHASE")
            logger.info("="*50)

            # Define API queries for comprehensive coverage
            api_queries = [
                # Books and technical manuals
                ("openlibrary", "computer science programming", 20),
                ("openlibrary", "artificial intelligence", 20),
                ("google_books", "machine learning", 10),
                ("internet_archive", "technical manuals", 15),

                # Music and audio content
                ("freemusicarchive", "electronic", 10),
                ("freemusicarchive", "classical", 10),

                # Academic content
                ("arxiv", "computer science", 15),
                ("pubmed", "medical research", 10),
            ]

            for api_name, query, limit in api_queries:
                if total_ingested >= max_items:
                    break

                logger.info(f"Querying {api_name}: '{query}' (limit: {limit})")
                stats = engine.ingest_from_api(api_name, query, max_items=limit)
                total_ingested += stats.total_ingested
                all_stats.append(stats)

                logger.info(f"  Results: {stats.total_ingested} ingested, {stats.duplicates_found} duplicates")

        # RSS Feed Ingestion
        if 'rss' in sources:
            logger.info("\n" + "="*50)
            logger.info("RSS FEED INGESTION PHASE")
            logger.info("="*50)

            # Define RSS feeds for podcasts, blogs, etc.
            rss_feeds = [
                # Podcasts
                "https://feeds.megaphone.fm/ADL9840290616",  # Some podcast feed
                # Add more RSS feeds as needed

                # Example feeds (these are placeholders - replace with real feeds)
                # "https://example.com/podcast/rss",
                # "https://example.com/blog/rss",
            ]

            if rss_feeds:
                stats = engine.ingest_from_rss(rss_feeds)
                total_ingested += stats.total_ingested
                all_stats.append(stats)
                logger.info(f"RSS Results: {stats.total_ingested} ingested from {stats.rss_feeds_processed} feeds")

        # Local Directory Ingestion
        if 'local' in sources:
            logger.info("\n" + "="*50)
            logger.info("LOCAL DIRECTORY INGESTION PHASE")
            logger.info("="*50)

            # Ingest from library directory
            library_dir = Path(library_path)
            if library_dir.exists():
                stats = engine.ingest_from_directory(str(library_dir))
                total_ingested += stats.total_ingested
                all_stats.append(stats)
                logger.info(f"Local Results: {stats.total_ingested} ingested from {stats.files_processed} files")

        # Summary
        processing_time = time.time() - start_time

        logger.info("\n" + "="*80)
        logger.info("INGESTION COMPLETE")
        logger.info("="*80)
        logger.info(f"Total ingested: {total_ingested}")
        logger.info(".2f")
        logger.info(".2f")

        # Detailed stats
        if all_stats:
            combined_stats = IngestionStats()
            for stats in all_stats:
                combined_stats.total_processed += stats.total_processed
                combined_stats.total_ingested += stats.total_ingested
                combined_stats.total_skipped += stats.total_skipped
                combined_stats.total_errors += stats.total_errors
                combined_stats.duplicates_found += stats.duplicates_found
                combined_stats.api_calls_made += stats.api_calls_made
                combined_stats.rss_feeds_processed += stats.rss_feeds_processed
                combined_stats.files_processed += stats.files_processed

            logger.info(f"Processing stats: {combined_stats.total_processed} processed, "
                       f"{combined_stats.duplicates_found} duplicates, "
                       f"{combined_stats.total_errors} errors")

        return total_ingested, processing_time

    except Exception as e:
        logger.error(f"Ingestion failed: {e}", exc_info=True)
        processing_time = time.time() - start_time
        return 0, processing_time

def construct_domain_knowledge_base(
    domain: str,
    source_texts: List[ContentMetadata],
    knowledge_base_path: str = "/knowledge"
) -> Dict[str, Any]:
    """
    Construct domain-specific knowledge base for LLM experts.

    This creates specialized knowledge bases under knowledge/ that can be
    dynamically loaded by LLM experts to build domain expertise.

    Args:
        domain: Domain name (e.g., 'classics', 'philosophy', 'literature')
        source_texts: List of ContentMetadata objects to process
        knowledge_base_path: Base path for knowledge bases

    Returns:
        Knowledge base metadata with quality metrics and expert profiles

    Example:
        >>> texts = [ContentMetadata(title="Plato's Republic", author="Plato", ...)]
        >>> kb = construct_domain_knowledge_base('classics', texts)
        >>> print(kb['expert_profiles']['plato_expert']['specialty'])
        Platonic Philosophy
    """
    try:
        logger.info(f"Constructing {domain} knowledge base with {len(source_texts)} texts")

        # Initialize constructor
        constructor = DomainKnowledgeBaseConstructor(domain, knowledge_base_path)

        # Build knowledge base
        kb_metadata = constructor.construct_knowledge_base(source_texts)

        logger.info(f"Successfully constructed {domain} knowledge base: "
                   f"{kb_metadata['total_texts']} texts, "
                   f"{len(kb_metadata['expert_profiles'])} expert profiles")

        return kb_metadata

    except Exception as e:
        logger.error(f"Failed to construct {domain} knowledge base: {e}", exc_info=True)
        return {}

def collect_documents(library_path: str) -> List[Any]:
    """
    Collect documents from library directory for ingestion.

    This is a legacy function for backward compatibility.
    """
    logger.warning("collect_documents() is deprecated. Use ingest_library() instead.")
    return []

# ============================================================================
# UNIFIED INGESTION MODES (Phase 2 Script Consolidation)
# ============================================================================

def ingest_from_library_mode(library_path: str = None) -> Tuple[int, float]:
    """
    Simple ingestion mode for backward compatibility with scripts/ingest_from_library.py.

    This replicates the functionality of the simple script but uses the enterprise engine
    for consistency and future enhancements.

    Args:
        library_path: Path to library directory (defaults to library/)

    Returns:
        Tuple of (total_ingested, processing_time_seconds)
    """
    start_time = time.time()

    # Default to library/ directory if not specified
    if library_path is None:
        library_path = get_config_value("files.library_path", "library")

    library_dir = Path(library_path)
    if not library_dir.exists():
        logger.error(f"Library path not found: {library_path}")
        return 0, time.time() - start_time

    logger.info(f"📚 Ingesting from library directory: {library_path}")

    # Initialize engine with simplified settings
    engine = EnterpriseIngestionEngine()

    # Use simple ingestion from directory only
    stats = engine.ingest_from_directory(str(library_dir))

    processing_time = time.time() - start_time

    logger.info("✨ Library ingestion complete!")
    logger.info(f"📊 Results: {stats.total_ingested} items ingested in {processing_time:.2f} seconds")

    return stats.total_ingested, processing_time

# ============================================================================
# CLI INTERFACE (Enhanced for Phase 2)
# ============================================================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Xoe-NovAi Library Ingestion Engine",
        epilog="""
EXAMPLES:
  # Enterprise multi-source ingestion (default)
  python -m app.XNAi_rag_app.ingest_library

  # Simple library directory ingestion (backward compatible)
  python -m app.XNAi_rag_app.ingest_library --mode from_library

  # Custom library path
  python -m app.XNAi_rag_app.ingest_library --mode from_library --library-path /custom/path

  # API-only ingestion
  python -m app.XNAi_rag_app.ingest_library --sources api --max-items 100
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    # Mode selection (Phase 2 addition)
    parser.add_argument("--mode", choices=['enterprise', 'from_library'],
                       default='enterprise',
                       help="Ingestion mode: enterprise (multi-source) or from_library (simple)")

    # Library-specific options (for from_library mode)
    parser.add_argument("--library-path", default=None,
                       help="Library directory path (for from_library mode)")

    # Enterprise options
    parser.add_argument("--batch-size", type=int, default=100,
                       help="Processing batch size (enterprise mode)")
    parser.add_argument("--max-items", type=int, default=1000,
                       help="Maximum items to process (enterprise mode)")
    parser.add_argument("--sources", nargs='+', choices=['api', 'rss', 'local'],
                       default=['api', 'rss', 'local'],
                       help="Sources to ingest from (enterprise mode)")
    parser.add_argument("--no-dedup", action="store_true",
                       help="Disable deduplication (enterprise mode)")
    parser.add_argument("--no-quality", action="store_true",
                       help="Disable quality filtering (enterprise mode)")

    args = parser.parse_args()

    # Route to appropriate function based on mode
    if args.mode == 'from_library':
        # Simple library ingestion mode (backward compatible)
        if args.sources != ['api', 'rss', 'local'] or args.batch_size != 100 or args.max_items != 1000:
            logger.warning("Additional enterprise options ignored in from_library mode")

        ingested, duration = ingest_from_library_mode(args.library_path)

    else:  # args.mode == 'enterprise'
        # Full enterprise ingestion
        ingested, duration = ingest_library(
            library_path=args.library_path,
            batch_size=args.batch_size,
            max_items=args.max_items,
            sources=args.sources,
            enable_deduplication=not args.no_dedup,
            enable_quality_filter=not args.no_quality
        )

    print(f"\nIngestion complete: {ingested} items processed in {duration:.2f} seconds")

# ============================================================================
# EXPORTS
# ============================================================================

__all__ = [
    # Core ingestion functionality
    "EnterpriseIngestionEngine",
    "ContentMetadata",
    "IngestionStats",
    "ingest_library",
    "collect_documents",

    # Scholarly text curation
    "ScholarlyTextCurator",
    "ScholarlyMetadata",

    # Domain knowledge bases
    "DomainKnowledgeBaseConstructor",
    "construct_domain_knowledge_base"
]
