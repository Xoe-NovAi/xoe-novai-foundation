# ============================================================================
# Xoe-NovAi Phase 1 v0.1.4-stable Application Configuration
# ============================================================================
# Purpose: Centralized application-level settings (complements .env runtime)
# Usage: Loaded by config_loader.py via toml.load() at startup
# Validation: python3 -c "import toml; toml.load('config.toml')"
# Guide Reference: Section 3.1 (config.toml Structure - 23 sections)
# Last Updated: 2025-11-08
# Sections: 23 total (v0.1.4-stable)
# ============================================================================

# ============================================================================
# [metadata] - Stack Identity
# ============================================================================
[metadata]
stack_version = "v0.1.0-alpha"
release_date = "2026-01-27"
codename = "Sovereign Foundation"
description = "CPU-optimized local AI RAG stack with CrawlModule v0.1.7"
architecture = "streaming-first, zero-telemetry, modular, library-curation"

# ============================================================================
# [project] - Core Settings
# ============================================================================
[project]
name = "Xoe-NovAi"
phase = 1
telemetry_enabled = false
privacy_mode = "local-only"
data_sovereignty = true
multi_agent_coordination = false

# ============================================================================
# [models] - LLM & Embedding Specifications
# ============================================================================
[models]
llm_path = "/models/smollm2-135m-instruct-q8_0.gguf"
llm_size_gb = 0.5
llm_quantization = "Q5_K_XL"
llm_context_window = 2048
embedding_path = "/embeddings/all-MiniLM-L12-v2.Q8_0.gguf"
embedding_size_mb = 45
embedding_dimensions = 384
embedding_model_name = "sentence-transformers/all-MiniLM-L12-v2"
embedding_device = "cpu"

# ============================================================================
# [performance] - Resource Limits & Targets
# ============================================================================
[performance]
token_rate_min = 15
token_rate_target = 20
token_rate_max = 25
# Memory limits in bytes (1 GB = 1073741824 bytes)
memory_limit_bytes = 5368709120  # 5.0 GB
memory_warning_threshold_bytes = 4831838208  # 4.5 GB
memory_critical_threshold_bytes = 5153960960  # 4.8 GB

# Legacy GB settings (DEPRECATED - use _bytes values above)
memory_limit_gb = 5.0  # DEPRECATED - minimum required by validation (5-32GB range)
memory_warning_threshold_gb = 4.5  # DEPRECATED
memory_critical_threshold_gb = 4.8  # DEPRECATED
latency_target_ms = 1000
latency_warning_ms = 1500
cpu_threads = 12
cpu_architecture = "AMD Ryzen 7 5700U (Zen2)"
f16_kv_enabled = true
per_doc_chars = 500
total_chars = 2048
debounce_seconds = 2
startup_timeout_s = 90
ingest_rate_min = 50
ingest_rate_target = 100
ingest_rate_max = 200
cache_hit_rate_target = 0.5
crawl_rate_target = 50

# ============================================================================
# [server] - FastAPI Configuration
# ============================================================================
[server]
host = "0.0.0.0"
port = 8000
workers = 1
max_body_size = "100MB"
timeout_seconds = 30
cors_origins = ["http://localhost:8001", "http://127.0.0.1:8001", "http://ui:8001"]
retry_attempts = 3
retry_backoff_factor = 2
retry_max_wait_s = 10

# ============================================================================
# [files] - Document Processing
# ============================================================================
[files]
max_size_mb = 100
accepted_types = ["md", "pdf", "txt"]
library_path = "/library"
knowledge_path = "/knowledge"
processing_timeout_s = 60
chunk_size = 1000
chunk_overlap = 200

# ============================================================================
# [session] - Session Management
# ============================================================================
[session]
session_timeout_s = 3600
max_concurrent_sessions = 10
session_persist = true
on_disk_payload = true

# ============================================================================
# [security] - Non-Root Configuration
# ============================================================================
[security]
non_root_uid = 1001
non_root_gid = 1001
non_root_user = "appuser"
drop_capabilities = "ALL"
add_capabilities = ["SETGID", "SETUID", "CHOWN"]
no_new_privileges = true

# ============================================================================
# [chainlit] - Chainlit UI Configuration
# ============================================================================
[chainlit]
host = "0.0.0.0"
port = 8001
no_telemetry = true
log_level = "INFO"
max_file_size = "100MB"
async_enabled = true
theme = "dark"
commands_enabled = true
session_persist = true

# ============================================================================
# [redis] - Redis 7.4.1 Configuration
# ============================================================================
[redis]
version = "7.4.1"
host = "redis"
port = 6379
password = "YOUR_REDIS_PASSWORD"
timeout_seconds = 60
max_connections = 50
appendonly = true
appendfsync = "everysec"
maxmemory = "512mb"
maxmemory_policy = "allkeys-lru"

[redis.streams]
coordination_stream = "xnai_coordination"
max_len = 1000

[redis.cache]
enabled = true
ttl_seconds = 3600
prefix = "xnai_cache"
max_keys = 10000
eviction_policy = "lru"
warmup_queries = [
  "What is Xoe-NovAi?",
  "How do I get started?",
  "What are the features?"
]

# ============================================================================
# [crawl] - CrawlModule v0.1.7 Configuration
# ============================================================================
[crawl]
enabled = true
version = "0.1.7"
max_depth = 2
rate_limit_per_min = 30
sanitize_scripts = true
max_items = 50
cache_ttl = 86400
user_agent = "Xoe-NovAi-CrawlModule/0.1.7"

[crawl.sources]
gutenberg = { enabled = true, priority = 1, url_pattern = ".gutenberg.org" }
arxiv = { enabled = true, priority = 2, url_pattern = ".arxiv.org" }
pubmed = { enabled = true, priority = 3, url_pattern = ".nih.gov" }
youtube = { enabled = true, priority = 4, url_pattern = ".youtube.com" }

[crawl.allowlist]
urls = [".gutenberg.org", ".arxiv.org", ".nih.gov", ".youtube.com"]
enforce = true

[crawl.metadata]
storage_path = "/knowledge/curator/index.toml"
track_sources = true
track_timestamps = true
track_categories = true

# ============================================================================
# [vectorstore] - FAISS Configuration
# ============================================================================
[vectorstore]
type = "faiss"
index_path = "/app/data/faiss_index"
backup_path = "/backups"
max_docs = 1000000
save_local_timeout_s = 30
ingest_batch_size = 100

[vectorstore.qdrant]
enabled = false
host = "localhost"
port = 6333
collection = "xnai_knowledge"
vector_size = 384
distance = "cosine"

# ============================================================================
# [api] - API Endpoints Configuration
# ============================================================================
[api]
base_url = "http://rag:8000"
health_endpoint = "/health"
query_endpoint = "/query"
stream_endpoint = "/stream"
curate_endpoint = "/curate"
streaming_chunk_size = 1024
streaming_heartbeat_s = 30
sse_keepalive_interval_s = 15
sse_max_message_size_bytes = 65536

# ============================================================================
# [logging] - Structured Logging Configuration
# ============================================================================
[logging]
level = "INFO"
format = "json"
file_enabled = true
file_path = "/app/XNAi_rag_app/logs/xnai.log"
max_size_mb = 10
backup_count = 5
console_enabled = true
include_timestamp = true
include_level = true
include_module = true
include_function = true

# ============================================================================
# [metrics] - Prometheus Configuration
# ============================================================================
[metrics]
enabled = true
port = 8002
multiproc_dir = "/prometheus_data"
update_interval_s = 30

[metrics.gauges]
memory_usage_gb = "Current memory usage in GB (system and process)"
token_rate_tps = "Token generation rate in tokens per second"
active_sessions = "Number of active user sessions"
cache_hit_rate = "Redis cache hit rate"

[metrics.histograms]
response_latency_ms = "API response latency in milliseconds"
rag_retrieval_time_ms = "RAG document retrieval time"
crawl_duration_ms = "CrawlModule operation duration"

[metrics.counters]
ingest_items_total = "Total items ingested"
crawl_items_total = "Total items crawled"
crawl_errors_total = "Total crawl errors"

# ============================================================================
# [healthcheck] - Health Monitoring Configuration
# ============================================================================
[healthcheck]
enabled = true
interval_seconds = 30
timeout_seconds = 15
retries = 5
start_period_seconds = 90
command = "python3 /app/XNAi_rag_app/healthcheck.py"
targets = ["llm", "embeddings", "memory", "redis", "vectorstore", "ryzen", "crawler", "telemetry"]
# Note: 8 total checks (all listed above, redis_streams is integrated into check_redis)

[healthcheck.thresholds]
memory_max_gb = 6.0
llm_response_timeout_s = 10
redis_ping_timeout_s = 5
vectorstore_search_timeout_s = 10
crawler_ping_timeout_s = 10

# ============================================================================
# [backup] - Backup & Recovery Configuration
# ============================================================================
[backup]
enabled = true
interval_hours = 24
retention_days = 7
backup_redis = true
backup_faiss = true
backup_logs = true
backup_path = "/backups"

[backup.faiss]
enabled = true
retention_days = 7
max_count = 5
cleanup_on_startup = true
compression = false
verify_on_load = true

# ============================================================================
# [phase2] - Multi-Agent Preparation (All disabled in Phase 1)
# ============================================================================
[phase2]
multi_agent_enabled = false
async_operations = true
max_concurrent_agents = 4
agent_task_queue_size = 100
agent_timeout_seconds = 300

[phase2.agents]
coding_assistant = { enabled = false, priority = 1, knowledge_path = "/knowledge/coder" }
library_curator = { enabled = false, priority = 2, knowledge_path = "/knowledge/curator" }
writing_assistant = { enabled = false, priority = 3, knowledge_path = "/knowledge/editor" }
project_manager = { enabled = false, priority = 4, knowledge_path = "/knowledge/manager" }
self_learning = { enabled = false, priority = 5, knowledge_path = "/knowledge/learner" }

# ============================================================================
# [docker] - Docker Configuration
# ============================================================================
[docker]
dockerfile_api = "./Dockerfile.api"
dockerfile_chainlit = "./Dockerfile.chainlit"
dockerfile_crawl = "./Dockerfile.crawl"
tmpfs_tmp_mode = "1777"
tmpfs_tmp_size = "512m"
tmpfs_chainlit_mode = "1777"
tmpfs_chainlit_size = "512m"
buildkit_enabled = true
no_cache = false

# ============================================================================
# [validation] - Validation Rules
# ============================================================================
[validation]
strict_mode = true
check_model_paths = true
check_env_vars = true
fail_on_missing_deps = true
max_memory_gb = 6.0
min_cpu_threads = 4
max_cpu_threads = 8
required_env_vars = 197
required_telemetry_disables = 8

# ============================================================================
# [debug] - Debug Settings (All disabled in production)
# ============================================================================
[debug]
mode = false
verbose_errors = false
profiling_enabled = false
trace_requests = false
log_sql_queries = false
mock_external_calls = false
dump_request_payloads = false
dump_response_payloads = false

# ============================================================================
# [voice] - Voice Interface Configuration (v0.1.5)
# ============================================================================
[voice]
enabled = true

# Wake Word Detection
wake_word = "hey grok"
wake_word_enabled = true
wake_word_sensitivity = 0.8

# STT Configuration (Updated for distil-large-v3-turbo - 2026 cutting-edge)
stt_provider = "faster_whisper"
stt_model = "distil-large-v3-turbo"  # Updated: 5x faster, 180-320ms latency
stt_device = "cpu"
stt_compute_type = "int8"  # Updated: CTranslate2 optimized for Ryzen
stt_beam_size = 5
stt_timeout_seconds = 30  # Reduced: faster model
vad_filter = true
vad_min_silence_duration_ms = 500
vad_provider = "silero"  # Added: ONNX-optimized VAD

# TTS Configuration
tts_provider = "piper_onnx"
piper_model = "en_US-john-medium"
tts_timeout_seconds = 30

# Input Validation & Rate Limiting
max_audio_size_bytes = 10485760  # 10MB
max_audio_duration_seconds = 300
rate_limit_per_minute = 10
rate_limit_window_seconds = 60

# Streaming Configuration
streaming_enabled = true
streaming_buffer_size = 4096
chunk_size_ms = 100

# Offline Mode
offline_mode = true
preload_models = false

# Cache Settings
enable_cache = true
cache_ttl_seconds = 3600
cache_max_entries = 1000
